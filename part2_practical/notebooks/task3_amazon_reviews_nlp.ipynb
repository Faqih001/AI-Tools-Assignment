{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b90c4a5d",
   "metadata": {},
   "source": [
    "# Task 3: NLP with spaCy - Amazon Product Reviews Analysis\n",
    "\n",
    "In this notebook, we'll analyze Amazon product reviews using spaCy to:\n",
    "1. Perform named entity recognition (NER) to extract product names and brands\n",
    "2. Analyze sentiment (positive/negative) using a rule-based approach\n",
    "\n",
    "## About the Task\n",
    "Natural Language Processing (NLP) is a key component of AI that enables machines to understand, interpret, and generate human language. In this task, we'll use spaCy, a powerful and efficient NLP library, to analyze product reviews."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ead7dd5",
   "metadata": {},
   "source": [
    "## 1. Import Required Libraries\n",
    "\n",
    "First, let's import all the libraries we'll need for our NLP analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17083a7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from collections import Counter, defaultdict\n",
    "import re\n",
    "from textblob import TextBlob\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Set random seed for reproducibility\n",
    "np.random.seed(42)\n",
    "\n",
    "# Set up the plotting style\n",
    "plt.style.use('seaborn-v0_8')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f114d24",
   "metadata": {},
   "source": [
    "## 2. Load and Set Up spaCy\n",
    "\n",
    "We'll load spaCy's English language model and check that it's working properly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd2671b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load English language model\n",
    "try:\n",
    "    nlp = spacy.load(\"en_core_web_sm\")\n",
    "    print(\"‚úÖ Successfully loaded spaCy English model\")\n",
    "    \n",
    "    # Check the model's capabilities\n",
    "    print(\"\\nModel capabilities:\")\n",
    "    print(f\"- Named Entity Recognition (NER): {'ner' in nlp.pipe_names}\")\n",
    "    print(f\"- Part-of-speech tagging: {'tagger' in nlp.pipe_names}\")\n",
    "    print(f\"- Dependency parsing: {'parser' in nlp.pipe_names}\")\n",
    "    print(f\"- Word vectors: {nlp.vocab.vectors_length > 0}\")\n",
    "    \n",
    "except OSError:\n",
    "    print(\"‚ùå spaCy English model not found. Please install it with:\")\n",
    "    print(\"python -m spacy download en_core_web_sm\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7a14561",
   "metadata": {},
   "source": [
    "## 3. Create Sample Amazon Reviews Dataset\n",
    "\n",
    "For this exercise, we'll create a sample dataset of Amazon product reviews. In a real-world scenario, you would load this data from an external source or API."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3960c21",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create sample Amazon product reviews for demonstration\n",
    "sample_reviews = [\n",
    "    \"I absolutely love my new iPhone 14 Pro from Apple! The camera quality is amazing and the battery life is fantastic. Highly recommend this product.\",\n",
    "    \"The Samsung Galaxy S23 is decent but the price is too high. The display is beautiful but I expected better performance for the cost.\",\n",
    "    \"These Nike Air Max shoes are incredibly comfortable. Perfect for running and daily wear. Great quality from Nike as always.\",\n",
    "    \"Bought this Sony PlayStation 5 and it's amazing! The graphics are stunning and the games load so fast. Sony really outdid themselves.\",\n",
    "    \"The MacBook Pro M2 from Apple is a powerhouse. Perfect for video editing and programming. The build quality is excellent as expected from Apple.\",\n",
    "    \"These Adidas sneakers are okay but not great. The design is nice but they're not as comfortable as my previous Nike shoes.\",\n",
    "    \"Love my new AirPods Pro! The noise cancellation is incredible. Apple continues to make amazing products.\",\n",
    "    \"The Dell laptop I bought has been disappointing. Poor build quality and the screen is not bright enough. Would not recommend Dell products.\",\n",
    "    \"This Kindle from Amazon is perfect for reading. The battery lasts forever and the screen is easy on the eyes. Great product from Amazon.\",\n",
    "    \"The Google Pixel 7 camera is outstanding! The night mode is incredible. Google has really improved their smartphone camera technology.\",\n",
    "    \"My new Tesla Model 3 is absolutely incredible! The autopilot feature is amazing and the battery range is impressive. Tesla is the future!\",\n",
    "    \"Disappointed with this Microsoft Surface Pro. The keyboard is flimsy and the performance is not as advertised. Expected better from Microsoft.\",\n",
    "    \"The Canon EOS R5 camera is a beast! Perfect for professional photography. The image quality is unmatched. Canon makes the best cameras.\",\n",
    "    \"These Bose headphones are worth every penny. The sound quality is phenomenal and the noise cancellation is top-notch. Bose is the best!\",\n",
    "    \"The LG OLED TV has stunning picture quality. The colors are vibrant and the contrast is perfect. LG makes excellent televisions.\",\n",
    "    \"Not happy with this HP printer. It's slow and the print quality is poor. Had better experience with Canon printers.\",\n",
    "    \"The Rolex Submariner is a masterpiece! The craftsmanship is incredible and it keeps perfect time. Rolex is truly luxury at its finest.\",\n",
    "    \"This BMW X5 is a fantastic SUV. The driving experience is smooth and the interior is luxurious. BMW engineering is outstanding.\",\n",
    "    \"The Nintendo Switch is perfect for gaming on the go. Great game library and the portability is excellent. Nintendo knows how to make fun consoles.\",\n",
    "    \"Disappointed with this Fitbit tracker. The battery life is poor and the app is buggy. My previous Garmin device was much better.\"\n",
    "]\n",
    "\n",
    "# Create DataFrame for better organization\n",
    "reviews_df = pd.DataFrame({\n",
    "    'review_id': range(1, len(sample_reviews) + 1),\n",
    "    'review_text': sample_reviews\n",
    "})\n",
    "\n",
    "print(f\"Created dataset with {len(sample_reviews)} sample reviews\")\n",
    "print(\"\\nFirst 5 reviews:\")\n",
    "reviews_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6540391",
   "metadata": {},
   "source": [
    "## 4. Basic Text Analysis\n",
    "\n",
    "Before diving into NER and sentiment analysis, let's do some basic text analysis to understand our data better."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b087af2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate basic text statistics\n",
    "reviews_df['word_count'] = reviews_df['review_text'].apply(lambda x: len(x.split()))\n",
    "reviews_df['char_count'] = reviews_df['review_text'].apply(len)\n",
    "\n",
    "print(\"Review text statistics:\")\n",
    "print(f\"- Average word count: {reviews_df['word_count'].mean():.2f} words per review\")\n",
    "print(f\"- Average character count: {reviews_df['char_count'].mean():.2f} characters per review\")\n",
    "print(f\"- Shortest review: {reviews_df['word_count'].min()} words\")\n",
    "print(f\"- Longest review: {reviews_df['word_count'].max()} words\")\n",
    "\n",
    "# Distribution of review lengths\n",
    "plt.figure(figsize=(12, 6))\n",
    "plt.hist(reviews_df['word_count'], bins=10, alpha=0.7, color='skyblue')\n",
    "plt.title('Distribution of Review Lengths')\n",
    "plt.xlabel('Word Count')\n",
    "plt.ylabel('Frequency')\n",
    "plt.grid(alpha=0.3)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c44b9c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Most common words (excluding stopwords)\n",
    "from collections import Counter\n",
    "import re\n",
    "\n",
    "def extract_words(text):\n",
    "    # Convert to lowercase and extract words\n",
    "    words = re.findall(r'\\b\\w+\\b', text.lower())\n",
    "    # Filter out short words (likely to be stopwords)\n",
    "    return [word for word in words if len(word) > 3]\n",
    "\n",
    "all_words = []\n",
    "for review in reviews_df['review_text']:\n",
    "    all_words.extend(extract_words(review))\n",
    "\n",
    "word_counts = Counter(all_words)\n",
    "most_common_words = word_counts.most_common(20)\n",
    "\n",
    "# Plot most common words\n",
    "plt.figure(figsize=(14, 8))\n",
    "words, counts = zip(*most_common_words)\n",
    "y_pos = np.arange(len(words))\n",
    "plt.barh(y_pos, counts, align='center', color='skyblue')\n",
    "plt.yticks(y_pos, words)\n",
    "plt.xlabel('Frequency')\n",
    "plt.title('Most Common Words in Reviews')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de965d2c",
   "metadata": {},
   "source": [
    "## 5. Named Entity Recognition (NER)\n",
    "\n",
    "Now let's use spaCy's NER capabilities to extract product names and brands from the reviews."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f77b4b45",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to extract entities from a review\n",
    "def extract_entities(text):\n",
    "    doc = nlp(text)\n",
    "    entities = []\n",
    "    for ent in doc.ents:\n",
    "        entities.append({\n",
    "            'text': ent.text,\n",
    "            'label': ent.label_,\n",
    "            'description': spacy.explain(ent.label_)\n",
    "        })\n",
    "    return entities\n",
    "\n",
    "# Apply entity extraction to all reviews\n",
    "reviews_df['entities'] = reviews_df['review_text'].apply(extract_entities)\n",
    "\n",
    "# Print an example\n",
    "print(\"Example of entity extraction:\")\n",
    "sample_idx = 0\n",
    "print(f\"\\nReview: {reviews_df['review_text'][sample_idx]}\")\n",
    "print(\"\\nExtracted entities:\")\n",
    "for ent in reviews_df['entities'][sample_idx]:\n",
    "    print(f\"- {ent['text']} ({ent['label']}: {ent['description']})\")\n",
    "\n",
    "# Analyze extracted entities\n",
    "all_entities = []\n",
    "for entities in reviews_df['entities']:\n",
    "    all_entities.extend(entities)\n",
    "\n",
    "print(f\"\\nTotal entities found: {len(all_entities)}\")\n",
    "entity_types = Counter([ent['label'] for ent in all_entities])\n",
    "print(\"\\nEntity types found:\")\n",
    "for ent_type, count in entity_types.most_common():\n",
    "    print(f\"- {ent_type} ({spacy.explain(ent_type)}): {count}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41ee38c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize entity types\n",
    "plt.figure(figsize=(12, 8))\n",
    "labels, counts = zip(*entity_types.most_common())\n",
    "plt.barh(range(len(counts)), counts, tick_label=labels)\n",
    "plt.title('Distribution of Entity Types')\n",
    "plt.xlabel('Count')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63b45e91",
   "metadata": {},
   "source": [
    "## 6. Enhanced Product and Brand Extraction with Pattern Matching\n",
    "\n",
    "While spaCy's NER is powerful, it might not catch all product names and brands in our specific domain. Let's supplement it with pattern matching to improve our extraction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3934426a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define patterns for brands and products\n",
    "brand_patterns = [\n",
    "    r'\\b(Apple|Samsung|Sony|Nike|Adidas|Google|Microsoft|Amazon|Tesla|Canon|Bose|LG|HP|Rolex|BMW|Nintendo|Fitbit|Garmin|Dell)\\b',\n",
    "]\n",
    "\n",
    "product_patterns = [\n",
    "    r'\\b(iPhone|Galaxy|PlayStation|MacBook|AirPods|Kindle|Pixel|Model [0-9]|Surface|EOS|Submariner|Switch)\\b',\n",
    "    r'\\b[A-Z][a-z]+ [A-Z][a-z]+ [0-9]+\\b',  # Product with model number\n",
    "    r'\\b[A-Z][a-z]+ [A-Z][0-9]+\\b',  # Product with alphanumeric model\n",
    "]\n",
    "\n",
    "def extract_with_patterns(text):\n",
    "    extracted_brands = set()\n",
    "    extracted_products = set()\n",
    "    \n",
    "    # Extract brands\n",
    "    for pattern in brand_patterns:\n",
    "        matches = re.findall(pattern, text, re.IGNORECASE)\n",
    "        extracted_brands.update(matches)\n",
    "    \n",
    "    # Extract products\n",
    "    for pattern in product_patterns:\n",
    "        matches = re.findall(pattern, text, re.IGNORECASE)\n",
    "        extracted_products.update(matches)\n",
    "    \n",
    "    return {\n",
    "        'brands': list(extracted_brands),\n",
    "        'products': list(extracted_products)\n",
    "    }\n",
    "\n",
    "# Apply pattern-based extraction\n",
    "reviews_df['pattern_extraction'] = reviews_df['review_text'].apply(extract_with_patterns)\n",
    "\n",
    "# Example of pattern-based extraction\n",
    "print(\"Example of pattern-based extraction:\")\n",
    "sample_idx = 0\n",
    "print(f\"\\nReview: {reviews_df['review_text'][sample_idx]}\")\n",
    "print(f\"\\nExtracted brands: {reviews_df['pattern_extraction'][sample_idx]['brands']}\")\n",
    "print(f\"Extracted products: {reviews_df['pattern_extraction'][sample_idx]['products']}\")\n",
    "\n",
    "# Collect all extracted brands and products\n",
    "all_brands = set()\n",
    "all_products = set()\n",
    "\n",
    "for extraction in reviews_df['pattern_extraction']:\n",
    "    all_brands.update(extraction['brands'])\n",
    "    all_products.update(extraction['products'])\n",
    "\n",
    "print(f\"\\nAll extracted brands: {sorted(all_brands)}\")\n",
    "print(f\"All extracted products: {sorted(all_products)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc998f3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyze brand mentions\n",
    "brand_mentions = Counter()\n",
    "for review in reviews_df['review_text']:\n",
    "    for brand in all_brands:\n",
    "        if brand.lower() in review.lower():\n",
    "            brand_mentions[brand] += 1\n",
    "\n",
    "# Plot brand mentions\n",
    "plt.figure(figsize=(12, 8))\n",
    "brands, counts = zip(*brand_mentions.most_common())\n",
    "y_pos = np.arange(len(brands))\n",
    "plt.barh(y_pos, counts, align='center', color='lightgreen')\n",
    "plt.yticks(y_pos, brands)\n",
    "plt.xlabel('Number of Mentions')\n",
    "plt.title('Brand Mentions in Reviews')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ffe861e0",
   "metadata": {},
   "source": [
    "## 7. Sentiment Analysis using Rule-Based Approach\n",
    "\n",
    "Now let's analyze the sentiment of each review using a rule-based approach. We'll use a combination of:\n",
    "1. Custom sentiment lexicons (positive/negative word lists)\n",
    "2. TextBlob for comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b47c086",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define sentiment lexicons\n",
    "positive_words = {\n",
    "    'love', 'amazing', 'fantastic', 'excellent', 'great', 'perfect', 'outstanding',\n",
    "    'incredible', 'wonderful', 'awesome', 'brilliant', 'superb', 'magnificent',\n",
    "    'phenomenal', 'impressive', 'recommend', 'best', 'good', 'beautiful'\n",
    "}\n",
    "\n",
    "negative_words = {\n",
    "    'hate', 'terrible', 'awful', 'horrible', 'disappointing', 'poor', 'bad',\n",
    "    'worst', 'disappointing', 'flimsy', 'slow', 'buggy', 'expensive', 'not happy',\n",
    "    'not recommend', 'not good', 'not great', 'not worth', 'waste'\n",
    "}\n",
    "\n",
    "def analyze_sentiment_rule_based(text):\n",
    "    # Process with spaCy\n",
    "    doc = nlp(text)\n",
    "    \n",
    "    # Rule-based sentiment scoring\n",
    "    positive_score = 0\n",
    "    negative_score = 0\n",
    "    positive_matches = []\n",
    "    negative_matches = []\n",
    "    \n",
    "    for token in doc:\n",
    "        word = token.lemma_.lower()\n",
    "        if word in positive_words:\n",
    "            positive_score += 1\n",
    "            positive_matches.append(token.text)\n",
    "        elif word in negative_words:\n",
    "            negative_score += 1\n",
    "            negative_matches.append(token.text)\n",
    "    \n",
    "    # Calculate sentiment using TextBlob for comparison\n",
    "    blob = TextBlob(text)\n",
    "    textblob_polarity = blob.sentiment.polarity\n",
    "    textblob_subjectivity = blob.sentiment.subjectivity\n",
    "    \n",
    "    # Determine overall sentiment\n",
    "    if positive_score > negative_score:\n",
    "        rule_sentiment = 'Positive'\n",
    "        rule_score = positive_score - negative_score\n",
    "    elif negative_score > positive_score:\n",
    "        rule_sentiment = 'Negative'\n",
    "        rule_score = negative_score - positive_score\n",
    "    else:\n",
    "        rule_sentiment = 'Neutral'\n",
    "        rule_score = 0\n",
    "    \n",
    "    # TextBlob sentiment\n",
    "    if textblob_polarity > 0.1:\n",
    "        textblob_sentiment = 'Positive'\n",
    "    elif textblob_polarity < -0.1:\n",
    "        textblob_sentiment = 'Negative'\n",
    "    else:\n",
    "        textblob_sentiment = 'Neutral'\n",
    "    \n",
    "    return {\n",
    "        'rule_sentiment': rule_sentiment,\n",
    "        'rule_score': rule_score,\n",
    "        'positive_words': positive_score,\n",
    "        'negative_words': negative_score,\n",
    "        'positive_matches': positive_matches,\n",
    "        'negative_matches': negative_matches,\n",
    "        'textblob_sentiment': textblob_sentiment,\n",
    "        'textblob_polarity': textblob_polarity,\n",
    "        'textblob_subjectivity': textblob_subjectivity\n",
    "    }\n",
    "\n",
    "# Apply sentiment analysis to all reviews\n",
    "reviews_df['sentiment'] = reviews_df['review_text'].apply(analyze_sentiment_rule_based)\n",
    "\n",
    "# Print an example\n",
    "print(\"Example of sentiment analysis:\")\n",
    "sample_idx = 0\n",
    "print(f\"\\nReview: {reviews_df['review_text'][sample_idx]}\")\n",
    "sentiment = reviews_df['sentiment'][sample_idx]\n",
    "print(f\"\\nRule-based sentiment: {sentiment['rule_sentiment']} (score: {sentiment['rule_score']})\")\n",
    "print(f\"TextBlob sentiment: {sentiment['textblob_sentiment']} (polarity: {sentiment['textblob_polarity']:.2f})\")\n",
    "print(f\"\\nPositive words found ({sentiment['positive_words']}): {sentiment['positive_matches']}\")\n",
    "print(f\"Negative words found ({sentiment['negative_words']}): {sentiment['negative_matches']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6fa1ad5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract sentiment results for analysis\n",
    "rule_sentiments = [s['rule_sentiment'] for s in reviews_df['sentiment']]\n",
    "textblob_sentiments = [s['textblob_sentiment'] for s in reviews_df['sentiment']]\n",
    "\n",
    "rule_counts = Counter(rule_sentiments)\n",
    "textblob_counts = Counter(textblob_sentiments)\n",
    "\n",
    "print(\"Rule-based Sentiment Distribution:\")\n",
    "for sentiment, count in rule_counts.items():\n",
    "    print(f\"- {sentiment}: {count} ({count/len(reviews_df)*100:.1f}%)\")\n",
    "\n",
    "print(\"\\nTextBlob Sentiment Distribution:\")\n",
    "for sentiment, count in textblob_counts.items():\n",
    "    print(f\"- {sentiment}: {count} ({count/len(reviews_df)*100:.1f}%)\")\n",
    "\n",
    "# Compare the two methods\n",
    "print(\"\\nAgreement between methods:\")\n",
    "agreement = sum(1 for r, t in zip(rule_sentiments, textblob_sentiments) if r == t)\n",
    "print(f\"- Reviews where both methods agree: {agreement} ({agreement/len(reviews_df)*100:.1f}%)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "401043d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize sentiment distribution\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(15, 6))\n",
    "\n",
    "# Rule-based sentiment\n",
    "labels = rule_counts.keys()\n",
    "sizes = rule_counts.values()\n",
    "colors = ['lightgreen', 'lightgray', 'salmon']\n",
    "ax1.pie(sizes, labels=labels, colors=colors, autopct='%1.1f%%', startangle=90)\n",
    "ax1.axis('equal')\n",
    "ax1.set_title('Rule-based Sentiment Distribution')\n",
    "\n",
    "# TextBlob sentiment\n",
    "labels = textblob_counts.keys()\n",
    "sizes = textblob_counts.values()\n",
    "ax2.pie(sizes, labels=labels, colors=colors, autopct='%1.1f%%', startangle=90)\n",
    "ax2.axis('equal')\n",
    "ax2.set_title('TextBlob Sentiment Distribution')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8416dc3f",
   "metadata": {},
   "source": [
    "## 8. Combined Analysis: Sentiment by Brand\n",
    "\n",
    "Now let's combine our entity extraction and sentiment analysis to understand sentiment across different brands."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68b234ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyze sentiment by brand\n",
    "brand_sentiment = defaultdict(list)\n",
    "\n",
    "for i, row in reviews_df.iterrows():\n",
    "    review_text = row['review_text'].lower()\n",
    "    sentiment = row['sentiment']['textblob_polarity']\n",
    "    \n",
    "    for brand in all_brands:\n",
    "        if brand.lower() in review_text:\n",
    "            brand_sentiment[brand].append(sentiment)\n",
    "\n",
    "# Calculate average sentiment for each brand\n",
    "brand_avg_sentiment = {}\n",
    "for brand, sentiments in brand_sentiment.items():\n",
    "    if sentiments:  # Check if we have sentiments for this brand\n",
    "        brand_avg_sentiment[brand] = sum(sentiments) / len(sentiments)\n",
    "\n",
    "# Sort brands by average sentiment\n",
    "sorted_brands = sorted(brand_avg_sentiment.items(), key=lambda x: x[1], reverse=True)\n",
    "\n",
    "# Plot brand sentiment\n",
    "plt.figure(figsize=(14, 8))\n",
    "brands = [b[0] for b in sorted_brands]\n",
    "sentiments = [b[1] for b in sorted_brands]\n",
    "\n",
    "colors = ['green' if s > 0 else 'red' for s in sentiments]\n",
    "plt.bar(brands, sentiments, color=colors)\n",
    "plt.axhline(y=0, color='gray', linestyle='-', alpha=0.3)\n",
    "plt.xlabel('Brand')\n",
    "plt.ylabel('Average Sentiment Score')\n",
    "plt.title('Average Sentiment by Brand')\n",
    "plt.xticks(rotation=45, ha='right')\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Print sentiment results by brand\n",
    "print(\"Brand Sentiment Analysis:\")\n",
    "for brand, avg_sentiment in sorted_brands:\n",
    "    sentiment_label = 'Positive' if avg_sentiment > 0.1 else 'Negative' if avg_sentiment < -0.1 else 'Neutral'\n",
    "    print(f\"- {brand}: {sentiment_label} (avg score: {avg_sentiment:.3f})\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f000e12",
   "metadata": {},
   "source": [
    "## 9. Advanced Analysis: Product Features and Sentiment\n",
    "\n",
    "Let's extract product features mentioned in reviews and analyze sentiment around these features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0182bb79",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define common product features\n",
    "product_features = {\n",
    "    'quality': ['quality', 'build', 'construction', 'durability'],\n",
    "    'performance': ['performance', 'speed', 'fast', 'responsive', 'slow', 'lag'],\n",
    "    'design': ['design', 'look', 'style', 'aesthetic', 'color'],\n",
    "    'price': ['price', 'cost', 'expensive', 'cheap', 'affordable', 'value'],\n",
    "    'battery': ['battery', 'charge', 'life', 'power'],\n",
    "    'display': ['screen', 'display', 'resolution', 'bright', 'color'],\n",
    "    'audio': ['sound', 'audio', 'speaker', 'noise', 'volume'],\n",
    "    'comfort': ['comfort', 'comfortable', 'fit', 'ergonomic']\n",
    "}\n",
    "\n",
    "# Function to extract features and associated sentiment\n",
    "def extract_feature_sentiment(text, doc=None):\n",
    "    if doc is None:\n",
    "        doc = nlp(text)\n",
    "    \n",
    "    feature_sentiments = defaultdict(list)\n",
    "    \n",
    "    # For each sentence in the document\n",
    "    for sent in doc.sents:\n",
    "        sent_text = sent.text.lower()\n",
    "        \n",
    "        # Check each feature category\n",
    "        for feature_cat, feature_terms in product_features.items():\n",
    "            for term in feature_terms:\n",
    "                if term in sent_text:\n",
    "                    # Calculate sentiment for this sentence\n",
    "                    sent_sentiment = TextBlob(sent.text).sentiment.polarity\n",
    "                    feature_sentiments[feature_cat].append(sent_sentiment)\n",
    "                    break  # Once we find a term in a category, move to next category\n",
    "    \n",
    "    # Average sentiment for each feature category\n",
    "    avg_feature_sentiment = {}\n",
    "    for feature, sentiments in feature_sentiments.items():\n",
    "        if sentiments:\n",
    "            avg_feature_sentiment[feature] = sum(sentiments) / len(sentiments)\n",
    "    \n",
    "    return avg_feature_sentiment\n",
    "\n",
    "# Apply feature sentiment extraction\n",
    "reviews_df['feature_sentiment'] = reviews_df['review_text'].apply(extract_feature_sentiment)\n",
    "\n",
    "# Aggregate feature sentiments across all reviews\n",
    "all_feature_sentiment = defaultdict(list)\n",
    "for feature_sent in reviews_df['feature_sentiment']:\n",
    "    for feature, sentiment in feature_sent.items():\n",
    "        all_feature_sentiment[feature].append(sentiment)\n",
    "\n",
    "# Calculate average sentiment for each feature\n",
    "avg_feature_sentiment = {}\n",
    "for feature, sentiments in all_feature_sentiment.items():\n",
    "    if sentiments:\n",
    "        avg_feature_sentiment[feature] = sum(sentiments) / len(sentiments)\n",
    "        \n",
    "# Sort features by average sentiment\n",
    "sorted_features = sorted(avg_feature_sentiment.items(), key=lambda x: x[1], reverse=True)\n",
    "\n",
    "# Plot feature sentiment\n",
    "plt.figure(figsize=(12, 7))\n",
    "features = [f[0] for f in sorted_features]\n",
    "sentiments = [f[1] for f in sorted_features]\n",
    "\n",
    "colors = ['green' if s > 0 else 'red' for s in sentiments]\n",
    "plt.bar(features, sentiments, color=colors)\n",
    "plt.axhline(y=0, color='gray', linestyle='-', alpha=0.3)\n",
    "plt.xlabel('Product Feature')\n",
    "plt.ylabel('Average Sentiment Score')\n",
    "plt.title('Average Sentiment by Product Feature')\n",
    "plt.xticks(rotation=45, ha='right')\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Print feature sentiment results\n",
    "print(\"Product Feature Sentiment Analysis:\")\n",
    "for feature, avg_sentiment in sorted_features:\n",
    "    sentiment_label = 'Positive' if avg_sentiment > 0.1 else 'Negative' if avg_sentiment < -0.1 else 'Neutral'\n",
    "    num_mentions = len(all_feature_sentiment[feature])\n",
    "    print(f\"- {feature}: {sentiment_label} (avg score: {avg_sentiment:.3f}, mentions: {num_mentions})\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b605fa43",
   "metadata": {},
   "source": [
    "## 10. Visualization Dashboard\n",
    "\n",
    "Let's create a comprehensive dashboard that summarizes our findings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c31729a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a comprehensive visualization dashboard\n",
    "plt.figure(figsize=(20, 16))\n",
    "\n",
    "# 1. Top brands by mention frequency\n",
    "plt.subplot(2, 2, 1)\n",
    "brands, counts = zip(*brand_mentions.most_common(8))\n",
    "plt.barh(range(len(brands)), counts, tick_label=brands)\n",
    "plt.title('Top Brands by Mention Frequency')\n",
    "plt.xlabel('Mentions')\n",
    "\n",
    "# 2. Sentiment distribution comparison\n",
    "plt.subplot(2, 2, 2)\n",
    "x = np.arange(3)\n",
    "width = 0.35\n",
    "\n",
    "sentiments = ['Negative', 'Neutral', 'Positive']\n",
    "rule_values = [rule_counts.get(s, 0) for s in sentiments]\n",
    "textblob_values = [textblob_counts.get(s, 0) for s in sentiments]\n",
    "\n",
    "plt.bar(x - width/2, rule_values, width, label='Rule-based', alpha=0.7)\n",
    "plt.bar(x + width/2, textblob_values, width, label='TextBlob', alpha=0.7)\n",
    "plt.xlabel('Sentiment')\n",
    "plt.ylabel('Count')\n",
    "plt.title('Sentiment Analysis Comparison')\n",
    "plt.xticks(x, sentiments)\n",
    "plt.legend()\n",
    "\n",
    "# 3. Feature sentiment analysis\n",
    "plt.subplot(2, 2, 3)\n",
    "features = [f[0] for f in sorted_features[:8]]  # Top 8 features\n",
    "sentiments = [f[1] for f in sorted_features[:8]]\n",
    "colors = ['green' if s > 0 else 'red' for s in sentiments]\n",
    "plt.bar(features, sentiments, color=colors)\n",
    "plt.axhline(y=0, color='gray', linestyle='-', alpha=0.3)\n",
    "plt.title('Product Feature Sentiment')\n",
    "plt.ylabel('Sentiment Score')\n",
    "plt.xticks(rotation=45, ha='right')\n",
    "\n",
    "# 4. Sentiment polarity distribution\n",
    "plt.subplot(2, 2, 4)\n",
    "polarities = [s['textblob_polarity'] for s in reviews_df['sentiment']]\n",
    "plt.hist(polarities, bins=20, color='skyblue', alpha=0.7)\n",
    "plt.title('Sentiment Polarity Distribution')\n",
    "plt.xlabel('Polarity Score')\n",
    "plt.ylabel('Frequency')\n",
    "plt.axvline(x=0, color='red', linestyle='--', alpha=0.7)\n",
    "\n",
    "plt.suptitle('Amazon Product Reviews Analysis Dashboard', fontsize=20)\n",
    "plt.tight_layout()\n",
    "plt.subplots_adjust(top=0.9)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "981c67c3",
   "metadata": {},
   "source": [
    "## 11. Generate Analysis Report\n",
    "\n",
    "Let's create a detailed report summarizing our findings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96b1975a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate most positive and negative reviews\n",
    "polarities = [s['textblob_polarity'] for s in reviews_df['sentiment']]\n",
    "most_positive_idx = polarities.index(max(polarities))\n",
    "most_negative_idx = polarities.index(min(polarities))\n",
    "\n",
    "# Most positive review\n",
    "print(\"‚≠ê MOST POSITIVE REVIEW:\")\n",
    "print(f\"Review ID: {reviews_df['review_id'][most_positive_idx]}\")\n",
    "print(f\"Polarity: {polarities[most_positive_idx]:.3f}\")\n",
    "print(f\"Text: {reviews_df['review_text'][most_positive_idx]}\")\n",
    "\n",
    "print(\"\\n\" + \"-\" * 50)\n",
    "\n",
    "# Most negative review\n",
    "print(\"\\n‚≠ê MOST NEGATIVE REVIEW:\")\n",
    "print(f\"Review ID: {reviews_df['review_id'][most_negative_idx]}\")\n",
    "print(f\"Polarity: {polarities[most_negative_idx]:.3f}\")\n",
    "print(f\"Text: {reviews_df['review_text'][most_negative_idx]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "183b7492",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate comprehensive analysis report\n",
    "print(\"=\" * 50)\n",
    "print(\"AMAZON PRODUCT REVIEWS NLP ANALYSIS REPORT\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "print(\"\\nüìä DATASET SUMMARY:\")\n",
    "print(f\"- Total reviews analyzed: {len(reviews_df)}\")\n",
    "print(f\"- Average review length: {reviews_df['word_count'].mean():.1f} words\")\n",
    "\n",
    "print(\"\\nüìä ENTITY EXTRACTION RESULTS:\")\n",
    "print(f\"- Total entities found: {len(all_entities)}\")\n",
    "print(f\"- Unique brands identified: {len(all_brands)}\")\n",
    "print(f\"- Unique products identified: {len(all_products)}\")\n",
    "print(f\"- Top 5 brands: {', '.join([b for b, _ in brand_mentions.most_common(5)])}\")\n",
    "\n",
    "print(\"\\nüìä SENTIMENT ANALYSIS RESULTS:\")\n",
    "print(\"Rule-based Sentiment Analysis:\")\n",
    "for sentiment in ['Positive', 'Neutral', 'Negative']:\n",
    "    count = rule_counts.get(sentiment, 0)\n",
    "    percentage = count / len(reviews_df) * 100\n",
    "    print(f\"  - {sentiment}: {count} reviews ({percentage:.1f}%)\")\n",
    "\n",
    "print(\"\\nTextBlob Sentiment Analysis:\")\n",
    "for sentiment in ['Positive', 'Neutral', 'Negative']:\n",
    "    count = textblob_counts.get(sentiment, 0)\n",
    "    percentage = count / len(reviews_df) * 100\n",
    "    print(f\"  - {sentiment}: {count} reviews ({percentage:.1f}%)\")\n",
    "\n",
    "print(\"\\nüìä BRAND SENTIMENT HIGHLIGHTS:\")\n",
    "for brand, avg_sentiment in sorted_brands[:3]:  # Top 3 positive brands\n",
    "    print(f\"  - Most positive brand: {brand} (score: {avg_sentiment:.3f})\")\n",
    "    break\n",
    "    \n",
    "for brand, avg_sentiment in reversed(sorted_brands[-3:]):  # Top 3 negative brands\n",
    "    print(f\"  - Most negative brand: {brand} (score: {avg_sentiment:.3f})\")\n",
    "    break\n",
    "\n",
    "print(\"\\nüìä PRODUCT FEATURE INSIGHTS:\")\n",
    "for feature, avg_sentiment in sorted_features[:3]:  # Top 3 positive features\n",
    "    print(f\"  - Most positive feature: {feature} (score: {avg_sentiment:.3f})\")\n",
    "    break\n",
    "    \n",
    "for feature, avg_sentiment in reversed(sorted_features[-3:]):  # Top 3 negative features\n",
    "    print(f\"  - Most negative feature: {feature} (score: {avg_sentiment:.3f})\")\n",
    "    break\n",
    "\n",
    "print(\"\\n\" + \"=\" * 50)\n",
    "print(\"END OF REPORT\")\n",
    "print(\"=\" * 50)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71884234",
   "metadata": {},
   "source": [
    "## 12. Conclusion\n",
    "\n",
    "We've successfully performed named entity recognition and sentiment analysis on Amazon product reviews using spaCy and rule-based approaches.\n",
    "\n",
    "### Summary of Achievements:\n",
    "1. **Entity Extraction**:\n",
    "   - Used spaCy's built-in NER to identify entities\n",
    "   - Enhanced product and brand extraction with custom pattern matching\n",
    "   - Identified key brands and products mentioned across reviews\n",
    "\n",
    "2. **Sentiment Analysis**:\n",
    "   - Implemented a rule-based sentiment analysis approach\n",
    "   - Compared results with TextBlob's sentiment analysis\n",
    "   - Analyzed sentiment by brand and product feature\n",
    "\n",
    "3. **Insights Generation**:\n",
    "   - Created visualizations to understand brand presence and sentiment\n",
    "   - Identified most positive and negative brands\n",
    "   - Analyzed sentiment around specific product features\n",
    "\n",
    "### Limitations and Future Improvements:\n",
    "1. **Enhanced NER**: Train a custom NER model specifically for product reviews\n",
    "2. **Aspect-Based Sentiment Analysis**: More granular sentiment analysis for specific product aspects\n",
    "3. **Larger Dataset**: Use a larger, more diverse dataset for better insights\n",
    "4. **Advanced ML Approaches**: Implement transformer-based models (BERT, etc.) for improved results\n",
    "\n",
    "### Business Value:\n",
    "This analysis demonstrates how NLP can extract valuable insights from customer reviews, helping businesses:\n",
    "- Identify strengths and weaknesses of products\n",
    "- Compare brand perception against competitors\n",
    "- Track sentiment across product features to guide improvements\n",
    "- Monitor customer satisfaction and respond to issues\n",
    "\n",
    "Overall, spaCy provides powerful NLP capabilities that can be enhanced with custom rules and approaches to gain valuable insights from customer feedback."
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
